{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"3a1073c2-d96c-4ab7-a267-9574940c9d2b","_cell_guid":"5103e74d-df63-452a-a868-ca25f34728f2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom math import *\nfrom IPython.display import clear_output\nimport random","metadata":{"_uuid":"4cf4f2d3-904a-4df3-a6ab-d2df76e6128e","_cell_guid":"324adc4b-84f1-4ba2-8b00-140c1ab961ea","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T10:31:13.847926Z","iopub.execute_input":"2024-06-16T10:31:13.848481Z","iopub.status.idle":"2024-06-16T10:31:13.856308Z","shell.execute_reply.started":"2024-06-16T10:31:13.848448Z","shell.execute_reply":"2024-06-16T10:31:13.855167Z"},"trusted":true},"execution_count":354,"outputs":[]},{"cell_type":"code","source":"try :\n  with open('/kaggle/working/input.txt', 'r', encoding='utf-8') as f:\n      text = f.read()\nexcept:\n  !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n  with open('input.txt', 'r', encoding='utf-8') as f:\n      text = f.read()\nprint(len(text))\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.1*len(data)) # first 90% will be train, rest val\ntrain_data = data[n:]\nval_data = data[:n]\n\n\"\"\"\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\"\"\"\n\n# data loading\ndef get_batch(split, block_size, batch_size):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n     ","metadata":{"execution":{"iopub.status.busy":"2024-06-16T10:31:14.171440Z","iopub.execute_input":"2024-06-16T10:31:14.171957Z","iopub.status.idle":"2024-06-16T10:31:14.489266Z","shell.execute_reply.started":"2024-06-16T10:31:14.171915Z","shell.execute_reply":"2024-06-16T10:31:14.487980Z"},"trusted":true},"execution_count":355,"outputs":[{"name":"stdout","text":"1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"class SupervCausalLSTMMemory(nn.Module):\n    def __init__(self, inp_size, out_size):\n        super().__init__()\n        self.out_size = out_size\n        \n        self.w = torch.randn(inp_size, out_size)\n        self.causal = torch.zeros(1, inp_size, out_size)\n        \n        self.deep = 1\n        self.outt_1 = torch.zeros(1, out_size)\n        \n        self.lstm = nn.LSTM(out_size, out_size, self.deep)\n        self.init_hid()\n        self.optim = torch.optim.AdamW(self.lstm.parameters(), lr=3e-2)\n        self.loss_lstm = nn.MSELoss()\n        \n    \n    def init_hid(self):\n        self.h = torch.zeros(self.deep, self.out_size)\n        self.c = torch.zeros(self.deep, self.out_size)\n        \n        self.h1 = torch.zeros(self.deep, self.out_size)\n        self.c1 = torch.zeros(self.deep, self.out_size)\n    \n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss\n    \n    def forward(self, sdr, target=None, learning=True, lr=3e-2):\n        \n        ########## FORWARD\n        # sdr : (1, inp_size)\n        \n\n        if learning:\n            out = target # (1, out_size)\n            causality = sdr.T @ out # (inp_size, out_size)\n            self.causal = torch.cat((self.causal, causality.unsqueeze(0)), axis=0)    \n            causal = torch.mean(self.causal, 0)\n            \n            loss = self.loss_fn(causal, self.w)\n            \n            self.w = self.w + (loss * lr)\n            \n        else:\n            out = torch.special.erf(sdr @ self.w) # (1, out_size)\n        \n        ########## PREDICTION\n\n        if learning:\n            pred_1, (self.h1, self.c1) = self.lstm(self.outt_1, (self.h1, self.c1))\n            self.h1, self.c1 = self.h1.detach(), self.c1.detach()\n\n            loss = self.loss_lstm(pred_1, out)\n            self.optim.zero_grad(set_to_none=True)\n            loss.backward()\n            self.optim.step()\n        \n        self.outt_1 = out\n        \n        pred, (self.h, self.c) = self.lstm(out, (self.h, self.c))\n        \n        return out, pred","metadata":{"_uuid":"6cfb0fec-aeea-4fd6-8d0f-25af568b8874","_cell_guid":"8fea39b2-4c15-468e-add5-87c42ab7e0e2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T10:31:21.094763Z","iopub.execute_input":"2024-06-16T10:31:21.095292Z","iopub.status.idle":"2024-06-16T10:31:21.110843Z","shell.execute_reply.started":"2024-06-16T10:31:21.095256Z","shell.execute_reply":"2024-06-16T10:31:21.109754Z"},"trusted":true},"execution_count":357,"outputs":[]},{"cell_type":"code","source":"class CausalMemory(nn.Module):\n    def __init__(self, inp_size, out_size, time_pred=3):\n        super().__init__()\n        self.time_pred = time_pred\n        self.out_size = out_size\n        \n        self.w = torch.randn(inp_size, out_size)\n        self.causal = torch.zeros(1, inp_size, out_size)\n        \n        self.wp = torch.randn(out_size, time_pred)\n        self.memory = []\n        self.causal_pred = torch.zeros(1, out_size, time_pred)\n        self.memoryt1 = torch.zeros(1, time_pred+1)\n\n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss\n    \n    def forward(self, sdr, learning=True, lr=3e-2):\n        \n        ########## FORWARD\n        # sdr : (1, inp_size)\n        out = torch.special.erf(sdr @ self.w) # (1, out_size)\n\n        if learning:\n            causality = sdr.T @ out # (inp_size, out_size)\n            self.causal = torch.cat((self.causal, causality.unsqueeze(0)), axis=0)    \n            causal = torch.mean(self.causal, 0)\n            \n            loss = self.loss_fn(causal, self.w)\n            \n            self.w = self.w + (loss * lr)\n        \n        self.memory.append(out.T) # (out_size, 1)\n        \n        ########## PREDICTION\n        \n        memory = torch.cat(self.memory, axis=1)[:, -self.time_pred:] # (out_size, time_pred)\n        \n        if self.memoryt1.shape[1] == self.time_pred:\n            pred = torch.sum(memory*self.wp, dim=1) # (out_size)\n            pred = torch.special.erf(pred).unsqueeze(1) # (out_size, 1)\n\n            if learning:\n                causality = out.T * self.memoryt1 # (out_size, time_pred)\n                self.causal_pred = torch.cat((self.causal_pred, causality.unsqueeze(0)), axis=0)    \n                causal = torch.mean(self.causal_pred, 0) # (out_size, time_pred)\n                \n                loss = self.loss_fn(causal, self.wp)# (out_size, time_pred)\n                self.wp = self.wp + (loss * lr) # (out_size, time_pred)\n        else:\n            pred = None\n        \n        self.memoryt1 = memory\n        \n        return out, pred","metadata":{"execution":{"iopub.status.busy":"2024-06-16T10:31:21.850143Z","iopub.execute_input":"2024-06-16T10:31:21.850528Z","iopub.status.idle":"2024-06-16T10:31:21.865154Z","shell.execute_reply.started":"2024-06-16T10:31:21.850500Z","shell.execute_reply":"2024-06-16T10:31:21.863946Z"},"trusted":true},"execution_count":358,"outputs":[]},{"cell_type":"code","source":"class SupervisedCausalMemory(nn.Module):\n    def __init__(self, inp_size, out_size, time_pred):\n        super().__init__()\n        self.time_pred = time_pred\n        self.out_size = out_size\n        \n        self.w = torch.randn(inp_size, out_size)\n        self.causal = torch.zeros(1, inp_size, out_size)\n        \n        self.wp = torch.randn(out_size, time_pred)\n        self.memory = []\n        self.causal_pred = torch.zeros(1, out_size, time_pred)\n        self.pred_countdown = 0\n        self.memoryt1 = torch.zeros(1, time_pred+1)\n        \n\n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss\n    \n    def forward(self, sdr, target=None, learning=True, lr=3e-2, pred_countdown=20):\n        # sdr : (1, inp_size)\n        # target : (1, out_size)\n        \n        ########## FORWARD\n        \n        out = torch.special.erf(sdr @ self.w) # (1, out_size)\n\n        if learning:\n            causality = sdr.T @ target # (inp_size, out_size)\n            self.causal = torch.cat((self.causal, causality.unsqueeze(0)), axis=0)    \n            causal = torch.mean(self.causal, 0)\n            \n            loss = self.loss_fn(causal, self.w)\n            \n            self.w = self.w + (loss * lr)\n        \n        self.memory.append(target.T if target is not None else out.T) # (out_size, 1)\n        \n        \n        ########## PREDICTION\n        \n        memory = torch.cat(self.memory, axis=1)[:, -self.time_pred:] # (out_size, time_pred)\n        \n        if memory.shape[1] == self.time_pred:\n            pred = torch.sum(memory*self.wp, dim=1) # (out_size)\n            pred = torch.special.erf(pred).unsqueeze(1) # (out_size, 1)\n            \n            if learning and self.pred_countdown > pred_countdown:\n                causality = target.T * self.memoryt1 # (out_size, time_pred)\n                \n                self.causal_pred = torch.cat((self.causal_pred, causality.unsqueeze(0)), axis=0)    \n                causal = torch.mean(self.causal_pred, 0) # (out_size, time_pred)\n                \n                loss = self.loss_fn(causal, self.wp) # (out_size, time_pred)\n    \n                self.wp = self.wp + (loss * lr) # (out_size, time_pred)\n                \n            self.memoryt1 = memory\n        else:\n            pred = None\n        \n        self.pred_countdown += 1\n        return out, pred.T if pred is not None else pred","metadata":{"execution":{"iopub.status.busy":"2024-06-16T10:31:25.740085Z","iopub.execute_input":"2024-06-16T10:31:25.740567Z","iopub.status.idle":"2024-06-16T10:31:25.757447Z","shell.execute_reply.started":"2024-06-16T10:31:25.740531Z","shell.execute_reply":"2024-06-16T10:31:25.756116Z"},"trusted":true},"execution_count":359,"outputs":[]},{"cell_type":"code","source":"data = [0, 1, 2, 3, 4, 5]\nl = SupervisedCausalMemory(6, 6, time_pred=4)\nfor i in range(500):\n    x = torch.full((1, 6), -1.0)\n    x[:, data[int(i%6)] ] = 1.0\n    out = l(x, x, lr=0.1)","metadata":{"_uuid":"c5a20ad9-dfa3-4bbe-b727-57ec5f3a6642","_cell_guid":"4ff3f001-4bce-4ed7-a9f7-fc26c1ec536e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T10:31:27.992123Z","iopub.execute_input":"2024-06-16T10:31:27.992513Z","iopub.status.idle":"2024-06-16T10:31:28.172377Z","shell.execute_reply.started":"2024-06-16T10:31:27.992483Z","shell.execute_reply":"2024-06-16T10:31:28.171302Z"},"trusted":true},"execution_count":360,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor([[-1, -1, 1, -1, -1, -1]]).float()\n\nout, pred = l(x, target=None, learning=False)\n","metadata":{"_uuid":"68b5cdfb-44c4-4b1a-90a9-c666a5a2d958","_cell_guid":"db2efdf9-2d6a-4fb4-bc41-75ce953633cd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T10:31:30.491153Z","iopub.execute_input":"2024-06-16T10:31:30.491526Z","iopub.status.idle":"2024-06-16T10:31:30.498684Z","shell.execute_reply.started":"2024-06-16T10:31:30.491496Z","shell.execute_reply":"2024-06-16T10:31:30.497377Z"},"trusted":true},"execution_count":361,"outputs":[]},{"cell_type":"code","source":"print(F.relu(out),\"\\n\", F.relu(pred))","metadata":{"_uuid":"36e6429b-b6d6-4ce3-8bda-67a2ddb95c44","_cell_guid":"e306dffb-ab66-4e88-8b1c-6ec38a6c4854","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T10:31:30.735336Z","iopub.execute_input":"2024-06-16T10:31:30.735739Z","iopub.status.idle":"2024-06-16T10:31:30.743672Z","shell.execute_reply.started":"2024-06-16T10:31:30.735709Z","shell.execute_reply":"2024-06-16T10:31:30.742362Z"},"trusted":true},"execution_count":362,"outputs":[{"name":"stdout","text":"tensor([[0., 0., 0., 0., 0., 0.]]) \n tensor([[0., 0., 0., 0., 0., 0.]])\n","output_type":"stream"}]},{"cell_type":"code","source":"class CausalBlock(nn.Module):\n    def __init__(self, dim, out_size, time_pred, lr=0.05):\n        super().__init__()  \n        self.lr = lr\n        \n        self.layers = nn.ModuleList([CausalMemory(dim[i], dim[i+1], time_pred=time_pred) for i in range(len(dim)-1)])\n        self.head = SupervisedCausalMemory(dim[-1], out_size, time_pred)\n        \n    def forward(self, x, target=None, learning=True):\n\n        for l in self.layers:\n            x, pred = l(x, learning=learning, lr=self.lr)\n\n        out, pred = self.head(x, target=target, learning=learning, lr=self.lr, pred_countdown=30)\n        return out, pred","metadata":{"execution":{"iopub.status.busy":"2024-06-16T10:31:31.975952Z","iopub.execute_input":"2024-06-16T10:31:31.976328Z","iopub.status.idle":"2024-06-16T10:31:31.984373Z","shell.execute_reply.started":"2024-06-16T10:31:31.976300Z","shell.execute_reply":"2024-06-16T10:31:31.983206Z"},"trusted":true},"execution_count":363,"outputs":[]},{"cell_type":"code","source":"def train():\n    len_pred = 10\n    #[vocab_size, int(1.2*vocab_size), int(1.4*vocab_size), int(1.6*vocab_size)]\n    model = CausalBlock([vocab_size], vocab_size, len_pred)\n    \n    for e in range(75):\n        idx = 0\n        clear_output(wait=True)\n        print(decode(train_data[0:len_pred].tolist()))\n        for i in range(len_pred):\n            letter = train_data[idx]\n            #print(itos[letter.item()], end=\"\")\n            x = torch.zeros(1, vocab_size, dtype=torch.float)\n            x[0, letter] = 1.0\n\n            out, pred = model(x, x)\n            try :\n                pass\n                max = torch.argmax(pred)\n                print(itos[max.item()], end=\"\")\n            except:\n                pass\n            idx += 1\n","metadata":{"_uuid":"6abaf920-8ab6-4157-8f3c-50924958380f","_cell_guid":"ca29bd67-37a7-4b80-ae63-86177fd366e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T10:31:32.448305Z","iopub.execute_input":"2024-06-16T10:31:32.448713Z","iopub.status.idle":"2024-06-16T10:31:32.457721Z","shell.execute_reply.started":"2024-06-16T10:31:32.448685Z","shell.execute_reply":"2024-06-16T10:31:32.456416Z"},"trusted":true},"execution_count":364,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T10:31:40.368775Z","iopub.execute_input":"2024-06-16T10:31:40.369174Z","iopub.status.idle":"2024-06-16T10:31:43.299461Z","shell.execute_reply.started":"2024-06-16T10:31:40.369144Z","shell.execute_reply":"2024-06-16T10:31:43.298030Z"},"trusted":true},"execution_count":365,"outputs":[{"name":"stdout","text":"et, Marciu\nt, Marciue","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}