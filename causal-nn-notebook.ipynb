{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"3a1073c2-d96c-4ab7-a267-9574940c9d2b","_cell_guid":"5103e74d-df63-452a-a868-ca25f34728f2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T16:39:09.596600Z","iopub.execute_input":"2024-06-16T16:39:09.597795Z","iopub.status.idle":"2024-06-16T16:39:09.605820Z","shell.execute_reply.started":"2024-06-16T16:39:09.597748Z","shell.execute_reply":"2024-06-16T16:39:09.604255Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom math import *\nfrom IPython.display import clear_output\nimport random","metadata":{"_uuid":"4cf4f2d3-904a-4df3-a6ab-d2df76e6128e","_cell_guid":"324adc4b-84f1-4ba2-8b00-140c1ab961ea","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T16:39:09.887011Z","iopub.execute_input":"2024-06-16T16:39:09.887407Z","iopub.status.idle":"2024-06-16T16:39:13.302213Z","shell.execute_reply.started":"2024-06-16T16:39:09.887376Z","shell.execute_reply":"2024-06-16T16:39:13.300981Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"try :\n  with open('/kaggle/working/input.txt', 'r', encoding='utf-8') as f:\n      text = f.read()\nexcept:\n  !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n  with open('input.txt', 'r', encoding='utf-8') as f:\n      text = f.read()\nprint(len(text))\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.1*len(data)) # first 90% will be train, rest val\ntrain_data = data[n:]\nval_data = data[:n]\n\n\"\"\"\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\"\"\"\n\n# data loading\ndef get_batch(split, block_size, batch_size):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n     ","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:39:13.304351Z","iopub.execute_input":"2024-06-16T16:39:13.304877Z","iopub.status.idle":"2024-06-16T16:39:15.060470Z","shell.execute_reply.started":"2024-06-16T16:39:13.304843Z","shell.execute_reply":"2024-06-16T16:39:15.058716Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-06-16 16:39:14--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n\n2024-06-16 16:39:14 (17.9 MB/s) - 'input.txt' saved [1115394/1115394]\n\n1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"class CausalMemory(nn.Module):\n    def __init__(self, inp_size, out_size, time_pred=3):\n        super().__init__()\n        self.time_pred = time_pred\n        self.out_size = out_size\n        \n        self.w = torch.randn(inp_size, out_size)\n        self.causal = torch.zeros(1, inp_size, out_size)\n        \n        self.wp = torch.randn(out_size, time_pred)\n        self.memory = []\n        self.causal_pred = torch.zeros(1, out_size, time_pred)\n        self.memoryt1 = torch.zeros(1, time_pred+1)\n\n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss\n    \n    def forward(self, sdr, learning=True, lr=3e-2):\n        \n        ########## FORWARD\n        # sdr : (1, inp_size)\n        out = torch.special.erf(sdr @ self.w) # (1, out_size)\n\n        if learning:\n            causality = sdr.T @ out # (inp_size, out_size)\n            self.causal = torch.cat((self.causal, causality.unsqueeze(0)), axis=0)    \n            causal = torch.mean(self.causal, 0)\n            \n            loss = self.loss_fn(causal, self.w)\n            \n            self.w = self.w + (loss * lr)\n        \n        self.memory.append(out.T) # (out_size, 1)\n        \n        ########## PREDICTION\n        \n        memory = torch.cat(self.memory, axis=1)[:, -self.time_pred:] # (out_size, time_pred)\n        \n        if self.memoryt1.shape[1] == self.time_pred:\n            pred = torch.sum(memory*self.wp, dim=1) # (out_size)\n            pred = torch.special.erf(pred).unsqueeze(1) # (out_size, 1)\n\n            if learning:\n                causality = out.T * self.memoryt1 # (out_size, time_pred)\n                self.causal_pred = torch.cat((self.causal_pred, causality.unsqueeze(0)), axis=0)    \n                causal = torch.mean(self.causal_pred, 0) # (out_size, time_pred)\n                \n                loss = self.loss_fn(causal, self.wp)# (out_size, time_pred)\n                self.wp = self.wp + (loss * lr) # (out_size, time_pred)\n        else:\n            pred = None\n        \n        self.memoryt1 = memory\n        \n        return out, pred","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:39:43.686920Z","iopub.execute_input":"2024-06-16T16:39:43.688621Z","iopub.status.idle":"2024-06-16T16:39:43.707024Z","shell.execute_reply.started":"2024-06-16T16:39:43.688570Z","shell.execute_reply":"2024-06-16T16:39:43.705415Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class SupervisedCausalMemory(nn.Module):\n    def __init__(self, inp_size, out_size, time_pred):\n        super().__init__()\n        self.time_pred = time_pred\n        self.out_size = out_size\n        \n        self.w = torch.randn(inp_size, out_size)\n        self.causal = torch.zeros(1, inp_size, out_size)\n        \n        self.wp = torch.randn(out_size, time_pred)\n        self.memory = []\n        self.causal_pred = torch.zeros(1, out_size, time_pred)\n        self.pred_countdown = 0\n        self.memoryt1 = torch.zeros(1, time_pred+1)\n        \n\n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss\n    \n    def forward(self, sdr, target=None, learning=True, lr=3e-2, pred_countdown=20):\n        # sdr : (1, inp_size)\n        # target : (1, out_size)\n        \n        ########## FORWARD\n        \n        out = torch.special.erf(sdr @ self.w) # (1, out_size)\n\n        if learning:\n            causality = sdr.T @ target # (inp_size, out_size)\n            self.causal = torch.cat((self.causal, causality.unsqueeze(0)), axis=0)    \n            causal = torch.mean(self.causal, 0)\n            \n            loss = self.loss_fn(causal, self.w)\n            \n            self.w = self.w + (loss * lr)\n        \n        self.memory.append(target.T if target is not None else out.T) # (out_size, 1)\n        \n        \n        ########## PREDICTION\n        \n        memory = torch.cat(self.memory, axis=1)[:, -self.time_pred:] # (out_size, time_pred)\n        \n        if memory.shape[1] == self.time_pred:\n            pred = torch.sum(memory*self.wp, dim=1) # (out_size)\n            pred = torch.special.erf(pred).unsqueeze(1) # (out_size, 1)\n            \n            if learning and self.pred_countdown > pred_countdown:\n                causality = target.T * self.memoryt1 # (out_size, time_pred)\n                \n                self.causal_pred = torch.cat((self.causal_pred, causality.unsqueeze(0)), axis=0)    \n                causal = torch.mean(self.causal_pred, 0) # (out_size, time_pred)\n                \n                loss = self.loss_fn(causal, self.wp) # (out_size, time_pred)\n    \n                self.wp = self.wp + (loss * lr) # (out_size, time_pred)\n                \n            self.memoryt1 = memory\n        else:\n            pred = None\n        \n        self.pred_countdown += 1\n        return out, pred.T if pred is not None else pred","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:39:44.631070Z","iopub.execute_input":"2024-06-16T16:39:44.631513Z","iopub.status.idle":"2024-06-16T16:39:44.650754Z","shell.execute_reply.started":"2024-06-16T16:39:44.631479Z","shell.execute_reply":"2024-06-16T16:39:44.649296Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = [0, 1, 2, 3, 4, 5]\nl = SupervisedCausalMemory(6, 6, time_pred=4)\nfor i in range(500):\n    x = torch.full((1, 6), -1.0)\n    x[:, data[int(i%6)] ] = 1.0\n    out = l(x, x, lr=0.1)","metadata":{"_uuid":"c5a20ad9-dfa3-4bbe-b727-57ec5f3a6642","_cell_guid":"4ff3f001-4bce-4ed7-a9f7-fc26c1ec536e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T16:39:45.439693Z","iopub.execute_input":"2024-06-16T16:39:45.440149Z","iopub.status.idle":"2024-06-16T16:39:45.732189Z","shell.execute_reply.started":"2024-06-16T16:39:45.440112Z","shell.execute_reply":"2024-06-16T16:39:45.730374Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor([[-1, -1, 1, -1, -1, -1]]).float()\n\nout, pred = l(x, target=None, learning=False)\n","metadata":{"_uuid":"68b5cdfb-44c4-4b1a-90a9-c666a5a2d958","_cell_guid":"db2efdf9-2d6a-4fb4-bc41-75ce953633cd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T16:39:51.634627Z","iopub.execute_input":"2024-06-16T16:39:51.635127Z","iopub.status.idle":"2024-06-16T16:39:51.642845Z","shell.execute_reply.started":"2024-06-16T16:39:51.635089Z","shell.execute_reply":"2024-06-16T16:39:51.641498Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(F.relu(out),\"\\n\", F.relu(pred))","metadata":{"_uuid":"36e6429b-b6d6-4ce3-8bda-67a2ddb95c44","_cell_guid":"e306dffb-ab66-4e88-8b1c-6ec38a6c4854","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T16:39:52.759779Z","iopub.execute_input":"2024-06-16T16:39:52.760229Z","iopub.status.idle":"2024-06-16T16:39:52.788241Z","shell.execute_reply.started":"2024-06-16T16:39:52.760193Z","shell.execute_reply":"2024-06-16T16:39:52.786881Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([[0., 0., 0., 0., 0., 0.]]) \n tensor([[0., 0., 0., 0., 0., 0.]])\n","output_type":"stream"}]},{"cell_type":"code","source":"class CausalBlock(nn.Module):\n    def __init__(self, dim, out_size, time_pred, lr=0.05):\n        super().__init__()  \n        self.lr = lr\n        \n        self.layers = nn.ModuleList([CausalMemory(dim[i], dim[i+1], time_pred=time_pred) for i in range(len(dim)-1)])\n        self.head = SupervisedCausalMemory(dim[-1], out_size, time_pred)\n        \n    def forward(self, x, target=None, learning=True):\n\n        for l in self.layers:\n            x, pred = l(x, learning=learning, lr=self.lr)\n\n        out, pred = self.head(x, target=target, learning=learning, lr=self.lr, pred_countdown=30)\n        return out, pred","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:39:55.499612Z","iopub.execute_input":"2024-06-16T16:39:55.500065Z","iopub.status.idle":"2024-06-16T16:39:55.511614Z","shell.execute_reply.started":"2024-06-16T16:39:55.500030Z","shell.execute_reply":"2024-06-16T16:39:55.510207Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train():\n    len_pred = 11\n    #[vocab_size, int(1.2*vocab_size), int(1.4*vocab_size), int(1.6*vocab_size)]\n    model = CausalBlock([vocab_size], vocab_size, len_pred)\n    \n    for e in range(75):\n        idx = 0\n        clear_output(wait=True)\n        print(decode(train_data[0:len_pred].tolist()))\n        for i in range(len_pred):\n            letter = train_data[idx]\n            #print(itos[letter.item()], end=\"\")\n            x = torch.zeros(1, vocab_size, dtype=torch.float)\n            x[0, letter] = 1.0\n\n            out, pred = model(x, x)\n            try :\n                pass\n                max = torch.argmax(pred)\n                print(itos[max.item()], end=\"\")\n            except:\n                pass\n            idx += 1\n","metadata":{"_uuid":"6abaf920-8ab6-4157-8f3c-50924958380f","_cell_guid":"ca29bd67-37a7-4b80-ae63-86177fd366e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-16T16:40:44.101769Z","iopub.execute_input":"2024-06-16T16:40:44.102803Z","iopub.status.idle":"2024-06-16T16:40:44.112599Z","shell.execute_reply.started":"2024-06-16T16:40:44.102705Z","shell.execute_reply":"2024-06-16T16:40:44.111347Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T16:40:44.681768Z","iopub.execute_input":"2024-06-16T16:40:44.682967Z","iopub.status.idle":"2024-06-16T16:40:47.942310Z","shell.execute_reply.started":"2024-06-16T16:40:44.682919Z","shell.execute_reply":"2024-06-16T16:40:47.941151Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"et, Marcius\nt, Marciuse","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}