{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-15T17:02:14.763801Z","iopub.execute_input":"2024-06-15T17:02:14.764288Z","iopub.status.idle":"2024-06-15T17:02:16.107393Z","shell.execute_reply.started":"2024-06-15T17:02:14.764251Z","shell.execute_reply":"2024-06-15T17:02:16.105809Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:02:16.110204Z","iopub.execute_input":"2024-06-15T17:02:16.110735Z","iopub.status.idle":"2024-06-15T17:02:20.582334Z","shell.execute_reply.started":"2024-06-15T17:02:16.110701Z","shell.execute_reply":"2024-06-15T17:02:20.580697Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class BatchedCausalLayer(nn.Module):\n    def __init__(self, inp_size, out_size, device='cpu'):\n        super().__init__()\n        \n        self.w = torch.randn(inp_size, out_size).to(device)\n        self.causal = torch.zeros(1, inp_size, out_size).to(device)\n        \n        self.wp = torch.randn(out_size, out_size).to(device)\n        self.outt_1 = None\n        self.causal_pred = torch.zeros(1, out_size, out_size).to(device)\n        \n        self.inertie = 1\n    \n    def forward(self, sdr, learning=True, lr=3e-2):\n        # sdr : (batch_size, inp_size)\n        \n        ####### Forward\n        out = torch.special.erf(torch.mm(sdr, self.w)) # (batch_size, out_size)\n        if torch.mean(torch.where(out > 0, 1, 0).float()) < 0.2:\n            print(\"no activation\")\n            for x in range(out.shape[0]):\n                for y in range(out.shape[1]):\n                    out[x, y] = 1 if out[x, y] < 0 else out[x, y]\n            \n        if learning:\n            sdr = sdr.unsqueeze(-1) # (b, inp, 1)\n            out_unsq = out.unsqueeze(1)  # (b, 1, inp)\n\n            causality = torch.special.erf(torch.bmm(sdr, out_unsq)) # (b, inp, out)\n            self.causal = torch.cat((self.causal[-int(self.inertie):, :, :], causality), axis=0).detach() # (50, inp, out)\n\n            causal = torch.mean(self.causal, 0) # (inp, out)\n\n            loss = self.loss_fn(causal, self.w)\n            self.w = self.w + (loss*lr)\n\n            \n        \n        \n        ####### Prediction \n        pred = torch.special.erf(torch.mm(out, self.wp)) # (batch_size, out_size)\n        \n        if learning:\n            pred = pred.unsqueeze(-1) # (b, out, 1)\n            outt_1 = self.outt_1.unsqueeze(1) if self.outt_1 is not None else torch.transpose(pred, 1, 2)  # (b, 1, out)\n            causality = torch.special.erf(torch.bmm(pred, outt_1)) # (b, out, out)\n            \n            self.causal_pred = torch.cat((self.causal_pred[-int(self.inertie):, :, :], causality), axis=0).detach() # (50, out, out)\n            causal = torch.mean(self.causal_pred, 0) # (out, out)\n            \n            loss = self.loss_fn(causal, self.wp)\n            self.wp = self.wp + (loss * lr)\n        \n        self.inertie = self.inertie*1.1 if self.inertie < 50 else self.inertie\n        self.outt_1 = out\n        return out, pred\n    \n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:02:24.303012Z","iopub.execute_input":"2024-06-15T17:02:24.303456Z","iopub.status.idle":"2024-06-15T17:02:24.323237Z","shell.execute_reply.started":"2024-06-15T17:02:24.303421Z","shell.execute_reply":"2024-06-15T17:02:24.321787Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"class CausalLayer():\n    def __init__(self, inp_size, out_size):\n        self.w = torch.randn(inp_size, out_size)\n        self.causal = torch.zeros(1, inp_size, out_size)\n        \n    \n    def forward(self, sdr, learning=True, lr=3e-2):\n        \n        ########## FORWARD\n        # sdr : (batch_size, inp_size)\n        out = torch.special.erf(sdr @ self.w) # (batch_size, out_size)\n\n        if learning:\n            causality = sdr.T @ out # (inp_size, out_size)\n            self.causal = torch.cat((self.causal, causality.unsqueeze(0)), axis=0)    \n            causal = torch.mean(self.causal, 0)\n            \n            loss = self.loss_fn(causal, self.w)\n            \n            self.w = self.w + (loss * lr)\n        \n        \n        ########## PREDICTION\n        \n        return out\n    \n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss","metadata":{}},{"cell_type":"code","source":"class CausalLSTMMemory(nn.Module):\n    def __init__(self, inp_size, out_size):\n        super().__init__()\n        self.out_size = out_size\n        \n        self.w = torch.randn(inp_size, out_size)\n        self.causal = torch.zeros(1, inp_size, out_size)\n        \n        self.deep = 1\n        self.outt_1 = torch.zeros(1, out_size)\n        \n        self.lstm = nn.LSTM(out_size, out_size, self.deep)\n        self.init_hid()\n        self.optim = torch.optim.AdamW(self.lstm.parameters(), lr=0.1)\n        self.loss_lstm = nn.MSELoss()\n        \n    \n    def init_hid(self):\n        self.h = torch.zeros(self.deep, self.out_size)\n        self.c = torch.zeros(self.deep, self.out_size)\n        \n        self.h1 = torch.zeros(self.deep, self.out_size)\n        self.c1 = torch.zeros(self.deep, self.out_size)\n    \n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss\n    \n    def forward(self, sdr, learning=True, lr=3e-2):\n        \n        ########## FORWARD\n        # sdr : (1, inp_size)\n        out = torch.special.erf(sdr @ self.w) # (1, out_size)\n\n        if learning:\n            causality = sdr.T @ out # (inp_size, out_size)\n            self.causal = torch.cat((self.causal, causality.unsqueeze(0)), axis=0)    \n            causal = torch.mean(self.causal, 0)\n            \n            loss = self.loss_fn(causal, self.w)\n            \n            self.w = self.w + (loss * lr)\n        \n        \n        ########## PREDICTION\n\n        if learning:\n            pred_1, (self.h1, self.c1) = self.lstm(self.outt_1, (self.h1, self.c1))\n            self.h1, self.c1 = self.h1.detach(), self.c1.detach()\n\n            loss = self.loss_lstm(pred_1, out)\n            self.optim.zero_grad(set_to_none=True)\n            loss.backward()\n            self.optim.step()\n        \n        self.outt_1 = out\n        \n        pred, (self.h, self.c) = self.lstm(out, (self.h, self.c))\n        \n        return out, pred\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:03:11.610495Z","iopub.execute_input":"2024-06-15T17:03:11.611027Z","iopub.status.idle":"2024-06-15T17:03:11.629005Z","shell.execute_reply.started":"2024-06-15T17:03:11.610962Z","shell.execute_reply":"2024-06-15T17:03:11.627504Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class SupervCausalLSTMMemory(nn.Module):\n    def __init__(self, inp_size, out_size):\n        super().__init__()\n        self.out_size = out_size\n        \n        self.w = torch.randn(inp_size, out_size)\n        self.causal = torch.zeros(1, inp_size, out_size)\n        \n        self.deep = 1\n        self.outt_1 = torch.zeros(1, out_size)\n        \n        self.lstm = nn.LSTM(out_size, out_size, self.deep)\n        self.init_hid()\n        self.optim = torch.optim.AdamW(self.lstm.parameters(), lr=0.1)\n        self.loss_lstm = nn.MSELoss()\n        \n    \n    def init_hid(self):\n        self.h = torch.zeros(self.deep, self.out_size)\n        self.c = torch.zeros(self.deep, self.out_size)\n        \n        self.h1 = torch.zeros(self.deep, self.out_size)\n        self.c1 = torch.zeros(self.deep, self.out_size)\n    \n    def loss_fn(self, a, b):\n        loss = a - b\n        return loss\n    \n    def forward(self, sdr, target=None, learning=True, lr=3e-2):\n        \n        ########## FORWARD\n        # sdr : (1, inp_size)\n        \n\n        if learning:\n            out = target # (1, out_size)\n            causality = sdr.T @ out # (inp_size, out_size)\n            self.causal = torch.cat((self.causal, causality.unsqueeze(0)), axis=0)    \n            causal = torch.mean(self.causal, 0)\n            \n            loss = self.loss_fn(causal, self.w)\n            \n            self.w = self.w + (loss * lr)\n            \n        else:\n            out = torch.special.erf(sdr @ self.w) # (1, out_size)\n        \n        ########## PREDICTION\n\n        if learning:\n            pred_1, (self.h1, self.c1) = self.lstm(self.outt_1, (self.h1, self.c1))\n            self.h1, self.c1 = self.h1.detach(), self.c1.detach()\n\n            loss = self.loss_lstm(pred_1, out)\n            self.optim.zero_grad(set_to_none=True)\n            loss.backward()\n            self.optim.step()\n        \n        self.outt_1 = out\n        \n        pred, (self.h, self.c) = self.lstm(out, (self.h, self.c))\n        \n        return out, pred\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:10:48.593730Z","iopub.execute_input":"2024-06-15T17:10:48.594161Z","iopub.status.idle":"2024-06-15T17:10:48.612591Z","shell.execute_reply.started":"2024-06-15T17:10:48.594127Z","shell.execute_reply":"2024-06-15T17:10:48.610939Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data = [0, 1, 2]\nl = CausalLSTMMemory(3, 5)\nsl = SupervCausalLSTMMemory(5, 3)\nfor i in range(500):\n    x = torch.full((1, 3), -1.0)\n    x[:, data[int(i%3)] ] = 1.0\n    out, pred = l(x, lr=0.1)\n    out, pred = sl(out, x, lr=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:10:52.636185Z","iopub.execute_input":"2024-06-15T17:10:52.636610Z","iopub.status.idle":"2024-06-15T17:10:54.984804Z","shell.execute_reply.started":"2024-06-15T17:10:52.636578Z","shell.execute_reply":"2024-06-15T17:10:54.983539Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor([[-1, -1, 1]]).float()\n\nout, pred = l(x, learning=False)\nout, pred = sl(out, learning=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:11:14.733636Z","iopub.execute_input":"2024-06-15T17:11:14.734096Z","iopub.status.idle":"2024-06-15T17:11:14.744530Z","shell.execute_reply.started":"2024-06-15T17:11:14.734062Z","shell.execute_reply":"2024-06-15T17:11:14.742827Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(F.relu(out), F.relu(pred))","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:12:01.499492Z","iopub.execute_input":"2024-06-15T17:12:01.500122Z","iopub.status.idle":"2024-06-15T17:12:01.512058Z","shell.execute_reply.started":"2024-06-15T17:12:01.500076Z","shell.execute_reply":"2024-06-15T17:12:01.510039Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tensor([[0.0000, 0.0000, 0.9871]]) tensor([[0.7589, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}